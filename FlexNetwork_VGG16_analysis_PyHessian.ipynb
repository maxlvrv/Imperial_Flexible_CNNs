{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #===================================================== Import libraries ================================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ================================================= Flexible Layer ================================================================================\n",
    "\n",
    "    \n",
    "class FlexiLayer(nn.Module): # class FlexiLayer(nn.Conv2d):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        super(FlexiLayer, self).__init__()\n",
    "        \n",
    "        self.t_1 = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.t_2 = nn.MaxPool2d(self.kernel_size, self.stride, self.padding) # get max result with the same kernel size\n",
    "        self.m = nn.Sigmoid()\n",
    "        \n",
    "        self.threshold1 = Variable(torch.randn((1, self.out_channels, 30, 30)))\n",
    "        \n",
    "        self.thresh_mean = []\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \n",
    "        self.threshold1.expand(t.size(0), self.out_channels, 30, 30)\n",
    "        \n",
    "        \n",
    "        cond = torch.sub(self.t_2(t), self.threshold1.cuda())\n",
    "        t_2_2 = self.m(cond*50)*self.t_2(t) # \n",
    "        t_1_1 = self.m(cond*(-50))*self.t_1(t) # \n",
    "        t = torch.add(t_2_2, t_1_1)\n",
    "        \n",
    "        return t\n",
    "\n",
    "    # ================================================= VGG-16 Network ================================================================================\n",
    "class VGG16(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG16,self).__init__()\n",
    "\n",
    "    self.block1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 3,out_channels = 64,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  nn.ReLU(),\n",
    "                  FlexiLayer(in_channels = 64,out_channels = 64,kernel_size = 3, padding =0),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  nn.ReLU(),\n",
    "                  #nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.3))\n",
    "\n",
    "    self.block2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 64,out_channels = 128,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(128),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 128,out_channels = 128,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(128),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block3 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 128,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block4 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2) ,\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block5 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.5) )\n",
    "\n",
    "    self.fc =     nn.Sequential(\n",
    "                  nn.Linear(512,100),\n",
    "                  nn.Dropout(0.5),\n",
    "                  nn.BatchNorm1d(100),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Dropout(0.5),\n",
    "                  nn.Linear(100,10), )\n",
    "                  \n",
    "                  \n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.block1(x)\n",
    "    out = self.block2(out)\n",
    "    out = self.block3(out)\n",
    "    out = self.block4(out)\n",
    "    out = self.block5(out)\n",
    "    out = out.view(out.size(0),-1)\n",
    "    out = self.fc(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CUDA Available:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): FlexiLayer(\n",
       "      (t_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (t_2): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "      (m): Sigmoid()\n",
       "    )\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Date: 10 Apr 2020\n",
    "\n",
    "Python version:      3.7\n",
    "PyTorch version:     1.2.0\n",
    "\n",
    "@author: Maksim Lavrov\n",
    "\n",
    "CIFAR10 dataset\n",
    "\n",
    "VGG16 baseline model\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "#from models.VGG16_with_flex_v5 import *\n",
    "\n",
    "# ======================================== prepare the dataset ==========================================================================================\n",
    "mean_cifar10 = [0.485, 0.456, 0.406]   \n",
    "std_cifar10 = [0.229, 0.224, 0.225]\n",
    "batch_size = 100\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=True, download= True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Define what device we are using\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda=True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# Initialize the network\n",
    "model = VGG16().to(device)\n",
    "\n",
    "# Load the pretrained model\n",
    "state = torch.load('./models/VGG16-flex-v5-block1-nnmodule_model_150_90.pth')\n",
    "model.load_state_dict(state['model'])\n",
    "\n",
    "# Set the model in evaluation mode. In this case this is for the Dropout layers\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fff /home/mlavrov/.local/share/jupyter/runtime/kernel-2227c2df-595e-4997-9ae7-20084e40ae3c.json\n",
      "mini_hessian_batch_size 200\n",
      "hessian_batch_size 200\n",
      "seed 1\n",
      "batch_norm True\n",
      "residual True\n",
      "cuda True\n",
      "resume fmnist_flex_model_14_52.pth\n",
      "********** finish data londing and begin Hessian computation **********\n",
      "\n",
      "***Top Eigenvalues:  [8090.9912109375]\n",
      "\n",
      "***Trace:  78703.48322860054\n"
     ]
    }
   ],
   "source": [
    "#*\n",
    "# @file Different utility functions\n",
    "# Copyright (c) Zhewei Yao, Amir Gholami\n",
    "# All rights reserved.\n",
    "# This file is part of PyHessian library.\n",
    "#\n",
    "# PyHessian is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# PyHessian is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with PyHessian.  If not, see <http://www.gnu.org/licenses/>.\n",
    "#*\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utils import *\n",
    "from density_plot import get_esd_plot\n",
    "from models.resnet import resnet\n",
    "from pyhessian import hessian\n",
    "\n",
    "# Settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "parser.add_argument(\n",
    "    '--mini-hessian-batch-size',\n",
    "    type=int,\n",
    "    default=200,\n",
    "    help='input batch size for mini-hessian batch (default: 200)')\n",
    "parser.add_argument('--hessian-batch-size',\n",
    "                    type=int,\n",
    "                    default=200,\n",
    "                    help='input batch size for hessian (default: 200)')\n",
    "parser.add_argument('--seed',\n",
    "                    type=int,\n",
    "                    default=1,\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--batch-norm',\n",
    "                    action='store_false',\n",
    "                    help='do we need batch norm or not')\n",
    "parser.add_argument('--residual',\n",
    "                    action='store_false',\n",
    "                    help='do we need residual connect or not')\n",
    "\n",
    "parser.add_argument('--cuda',\n",
    "                    action='store_false',\n",
    "                    help='do we use gpu or not')\n",
    "parser.add_argument('--resume',\n",
    "                    type=str,\n",
    "                    default='fmnist_flex_model_14_52.pth',\n",
    "                    help='get the checkpoint')\n",
    "\n",
    "args = parser.parse_args()\n",
    "# set random seed to reproduce the work\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "for arg in vars(args):\n",
    "    print(arg, getattr(args, arg))\n",
    "\n",
    "# get dataset\n",
    "#train_loader, test_loader = getData(name='cifar10_without_dataaugmentation',\n",
    "#                                    train_bs=args.mini_hessian_batch_size,\n",
    "#                                    test_bs=1)\n",
    "\n",
    "train_loader, test_loader = testloader, trainloader\n",
    "\n",
    "##############\n",
    "# Get the hessian data\n",
    "##############\n",
    "assert (args.hessian_batch_size % args.mini_hessian_batch_size == 0)\n",
    "assert (50000 % args.hessian_batch_size == 0)\n",
    "batch_num = args.hessian_batch_size // args.mini_hessian_batch_size\n",
    "\n",
    "if batch_num == 1:\n",
    "    for inputs, labels in train_loader:\n",
    "        hessian_dataloader = (inputs, labels)\n",
    "        break\n",
    "else:\n",
    "    hessian_dataloader = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        hessian_dataloader.append((inputs, labels))\n",
    "        if i == batch_num - 1:\n",
    "            break\n",
    "\n",
    "if args.cuda:\n",
    "    model = model.cuda()\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # label loss\n",
    "\n",
    "###################\n",
    "# Get model checkpoint, get saving folder\n",
    "###################\n",
    "if args.resume == '':\n",
    "    raise Exception(\"please choose the trained model\")\n",
    "#model.load_state_dict(torch.load(args.resume))\n",
    "#state = torch.load('fmnist_base_model_14_28.pth')\n",
    "#model.load_state_dict(state['model'])\n",
    "\n",
    "######################################################\n",
    "# Begin the computation\n",
    "######################################################\n",
    "\n",
    "# turn model to eval mode\n",
    "model.eval()\n",
    "if batch_num == 1:\n",
    "    hessian_comp = hessian(model,\n",
    "                           criterion,\n",
    "                           data=hessian_dataloader,\n",
    "                           cuda=args.cuda)\n",
    "else:\n",
    "    hessian_comp = hessian(model,\n",
    "                           criterion,\n",
    "                           dataloader=hessian_dataloader,\n",
    "                           cuda=args.cuda)\n",
    "\n",
    "print(\n",
    "    '********** finish data londing and begin Hessian computation **********')\n",
    "\n",
    "top_eigenvalues, _ = hessian_comp.eigenvalues()\n",
    "trace = hessian_comp.trace()\n",
    "density_eigen, density_weight = hessian_comp.density()\n",
    "\n",
    "print('\\n***Top Eigenvalues: ', top_eigenvalues)\n",
    "print('\\n***Trace: ', np.mean(trace))\n",
    "\n",
    "get_esd_plot(density_eigen, density_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
