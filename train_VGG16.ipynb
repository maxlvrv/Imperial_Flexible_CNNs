{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([3, 36, 138])\n",
      "truck  ship truck   car\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " #===================================================== Import libraries ================================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn \n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# =================================================== Prepare the dataset ===============================================================================\n",
    "\n",
    "mean_cifar10 = [0.485, 0.456, 0.406]  # Mean and Std value hase been taken from a github implmentation online.\n",
    "std_cifar10 = [0.229, 0.224, 0.225]\n",
    "batch_size = 100\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=True, download= True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') # 10 Classes of the cifar-10\n",
    "\n",
    "# ========================================== Visualising the dataset ==========================================================================\n",
    "std= torch.FloatTensor(std_cifar10)\n",
    "mean = torch.FloatTensor(mean_cifar10)\n",
    "mean = mean[:,None,None]\n",
    "std = std[:,None,None]\n",
    "def imshow(img):\n",
    "    print(img.size())\n",
    "    img = img*std + mean     # unnormalize\n",
    "    \n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:4]))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "# ================================================= VGG-16 Network ================================================================================\n",
    "class VGG16(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(VGG16,self).__init__()\n",
    "\n",
    "    self.block1 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 3,out_channels = 64,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 64,out_channels = 64,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(64),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.3))\n",
    "\n",
    "    self.block2 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 64,out_channels = 128,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(128),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 128,out_channels = 128,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(128),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block3 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 128,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 256,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(256),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block4 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 256,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2) ,\n",
    "                  nn.Dropout2d(0.4))\n",
    "\n",
    "    self.block5 = nn.Sequential(\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3,padding = 1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Conv2d(in_channels = 512,out_channels = 512,kernel_size = 3, padding =1),\n",
    "                  nn.BatchNorm2d(512),\n",
    "                  nn.ReLU(),\n",
    "                  nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                  nn.Dropout2d(0.5) )\n",
    "\n",
    "    self.fc =     nn.Sequential(\n",
    "                  nn.Linear(512,100),\n",
    "                  nn.Dropout(0.5),\n",
    "                  nn.BatchNorm1d(100),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Dropout(0.5),\n",
    "                  nn.Linear(100,10), )\n",
    "                  \n",
    "                  \n",
    "\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.block1(x)\n",
    "    out = self.block2(out)\n",
    "    out = self.block3(out)\n",
    "    out = self.block4(out)\n",
    "    out = self.block5(out)\n",
    "    out = out.view(out.size(0),-1)\n",
    "    out = self.fc(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14,775,646 total parameters.\n",
      "14,775,646 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================== Model initialisation, Loss function and Optimizer =====================================\n",
    "model = VGG16()\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = 0.001,momentum = 0.9,weight_decay = 0.006)\n",
    "schedule = torch.optim.lr_scheduler.StepLR(optimizer,step_size=20,gamma = 0.7)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} trainable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16(\n",
       "  (block1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout2d(p=0.3, inplace=False)\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.4, inplace=False)\n",
       "  )\n",
       "  (block5): Sequential(\n",
       "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlavrov/anaconda3/envs/meng/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0   Loss:  1097.43492269516   Train Accuracy : 15.802\n",
      "\n",
      "\n",
      "Test accuracy: 13 %\n",
      "Epoch:  1   Loss:  962.7349643707275   Train Accuracy : 24.314\n",
      "\n",
      "\n",
      "Test accuracy: 19 %\n",
      "Epoch:  2   Loss:  891.2292386293411   Train Accuracy : 29.864\n",
      "\n",
      "\n",
      "Test accuracy: 28 %\n",
      "Epoch:  3   Loss:  851.1047434806824   Train Accuracy : 33.798\n",
      "\n",
      "\n",
      "Test accuracy: 32 %\n",
      "Epoch:  4   Loss:  813.9939724206924   Train Accuracy : 37.386\n",
      "\n",
      "\n",
      "Test accuracy: 37 %\n",
      "Epoch:  5   Loss:  778.7494065761566   Train Accuracy : 40.666\n",
      "\n",
      "\n",
      "Test accuracy: 42 %\n",
      "Epoch:  6   Loss:  747.6973925828934   Train Accuracy : 43.352\n",
      "\n",
      "\n",
      "Test accuracy: 46 %\n",
      "Epoch:  7   Loss:  716.826714515686   Train Accuracy : 46.934\n",
      "\n",
      "\n",
      "Test accuracy: 52 %\n",
      "Epoch:  8   Loss:  684.7464009523392   Train Accuracy : 50.826\n",
      "\n",
      "\n",
      "Test accuracy: 58 %\n",
      "Epoch:  9   Loss:  657.891504406929   Train Accuracy : 54.024\n",
      "\n",
      "\n",
      "Test accuracy: 60 %\n",
      "Epoch:  10   Loss:  623.9461681246758   Train Accuracy : 57.504\n",
      "\n",
      "\n",
      "Test accuracy: 62 %\n",
      "Epoch:  11   Loss:  597.8178095817566   Train Accuracy : 59.9\n",
      "\n",
      "\n",
      "Test accuracy: 66 %\n",
      "Epoch:  12   Loss:  573.4367065429688   Train Accuracy : 61.822\n",
      "\n",
      "\n",
      "Test accuracy: 67 %\n",
      "Epoch:  13   Loss:  551.5962717533112   Train Accuracy : 63.448\n",
      "\n",
      "\n",
      "Test accuracy: 69 %\n",
      "Epoch:  14   Loss:  533.3581482768059   Train Accuracy : 65.39\n",
      "\n",
      "\n",
      "Test accuracy: 70 %\n",
      "Epoch:  15   Loss:  515.8255823254585   Train Accuracy : 67.126\n",
      "\n",
      "\n",
      "Test accuracy: 72 %\n",
      "Epoch:  16   Loss:  500.3864951133728   Train Accuracy : 68.398\n",
      "\n",
      "\n",
      "Test accuracy: 72 %\n",
      "Epoch:  17   Loss:  485.40351313352585   Train Accuracy : 69.872\n",
      "\n",
      "\n",
      "Test accuracy: 74 %\n",
      "Epoch:  18   Loss:  472.65921825170517   Train Accuracy : 70.612\n",
      "\n",
      "\n",
      "Test accuracy: 76 %\n",
      "Epoch:  19   Loss:  446.67665362358093   Train Accuracy : 72.684\n",
      "\n",
      "\n",
      "Test accuracy: 76 %\n",
      "Epoch:  20   Loss:  435.0947903394699   Train Accuracy : 73.916\n",
      "\n",
      "\n",
      "Test accuracy: 78 %\n",
      "Epoch:  21   Loss:  424.7401342391968   Train Accuracy : 74.696\n",
      "\n",
      "\n",
      "Test accuracy: 79 %\n",
      "Epoch:  22   Loss:  413.3843787908554   Train Accuracy : 75.234\n",
      "\n",
      "\n",
      "Test accuracy: 79 %\n",
      "Epoch:  23   Loss:  405.9593558907509   Train Accuracy : 76.044\n",
      "\n",
      "\n",
      "Test accuracy: 79 %\n",
      "Epoch:  24   Loss:  395.36702942848206   Train Accuracy : 76.726\n",
      "\n",
      "\n",
      "Test accuracy: 80 %\n",
      "Epoch:  25   Loss:  386.1749241054058   Train Accuracy : 77.42\n",
      "\n",
      "\n",
      "Test accuracy: 79 %\n",
      "Epoch:  26   Loss:  379.86077374219894   Train Accuracy : 78.03\n",
      "\n",
      "\n",
      "Test accuracy: 80 %\n",
      "Epoch:  27   Loss:  375.4422440826893   Train Accuracy : 78.48\n",
      "\n",
      "\n",
      "Test accuracy: 79 %\n",
      "Epoch:  28   Loss:  369.36734014749527   Train Accuracy : 78.922\n",
      "\n",
      "\n",
      "Test accuracy: 80 %\n",
      "Epoch:  29   Loss:  361.01303419470787   Train Accuracy : 79.622\n",
      "\n",
      "\n",
      "Test accuracy: 82 %\n",
      "Epoch:  30   Loss:  355.4098206460476   Train Accuracy : 79.968\n",
      "\n",
      "\n",
      "Test accuracy: 82 %\n",
      "Epoch:  31   Loss:  348.8656429052353   Train Accuracy : 80.302\n",
      "\n",
      "\n",
      "Test accuracy: 82 %\n",
      "Epoch:  32   Loss:  342.6187901198864   Train Accuracy : 80.758\n",
      "\n",
      "\n",
      "Test accuracy: 83 %\n",
      "Epoch:  33   Loss:  337.64107859134674   Train Accuracy : 81.286\n",
      "\n",
      "\n",
      "Test accuracy: 83 %\n",
      "Epoch:  34   Loss:  330.6740151941776   Train Accuracy : 81.814\n",
      "\n",
      "\n",
      "Test accuracy: 83 %\n",
      "Epoch:  35   Loss:  328.34705424308777   Train Accuracy : 81.974\n",
      "\n",
      "\n",
      "Test accuracy: 83 %\n",
      "Epoch:  36   Loss:  320.73475736379623   Train Accuracy : 82.368\n",
      "\n",
      "\n",
      "Test accuracy: 83 %\n",
      "Epoch:  37   Loss:  317.4496790766716   Train Accuracy : 82.734\n",
      "\n",
      "\n",
      "Test accuracy: 84 %\n",
      "Epoch:  38   Loss:  311.706072896719   Train Accuracy : 82.964\n",
      "\n",
      "\n",
      "Test accuracy: 84 %\n",
      "Epoch:  39   Loss:  292.25173729658127   Train Accuracy : 84.436\n",
      "\n",
      "\n",
      "Test accuracy: 84 %\n",
      "Epoch:  40   Loss:  284.6940589249134   Train Accuracy : 85.004\n",
      "\n",
      "\n",
      "Test accuracy: 85 %\n",
      "Epoch:  41   Loss:  277.06600719690323   Train Accuracy : 85.5\n",
      "\n",
      "\n",
      "Test accuracy: 84 %\n",
      "Epoch:  42   Loss:  274.994602650404   Train Accuracy : 85.45\n",
      "\n",
      "\n",
      "Test accuracy: 85 %\n",
      "Epoch:  43   Loss:  272.76377910375595   Train Accuracy : 85.684\n",
      "\n",
      "\n",
      "Test accuracy: 85 %\n",
      "Epoch:  44   Loss:  268.27105471491814   Train Accuracy : 85.954\n",
      "\n",
      "\n",
      "Test accuracy: 85 %\n",
      "Epoch:  45   Loss:  267.78035348653793   Train Accuracy : 86.1\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  46   Loss:  263.6640683412552   Train Accuracy : 86.342\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  47   Loss:  261.82113948464394   Train Accuracy : 86.41\n",
      "\n",
      "\n",
      "Test accuracy: 85 %\n",
      "Epoch:  48   Loss:  258.1240282058716   Train Accuracy : 86.668\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  49   Loss:  254.3674858212471   Train Accuracy : 86.902\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  50   Loss:  254.51930764317513   Train Accuracy : 86.992\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  51   Loss:  247.7123640179634   Train Accuracy : 87.314\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  52   Loss:  247.88334265351295   Train Accuracy : 87.472\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  53   Loss:  243.77277806401253   Train Accuracy : 87.512\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  54   Loss:  239.85347658395767   Train Accuracy : 87.962\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  55   Loss:  241.977463722229   Train Accuracy : 87.872\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  56   Loss:  234.48561899363995   Train Accuracy : 88.31\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  57   Loss:  236.9667388498783   Train Accuracy : 88.236\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  58   Loss:  233.26580679416656   Train Accuracy : 88.22\n",
      "\n",
      "\n",
      "Test accuracy: 86 %\n",
      "Epoch:  59   Loss:  214.17199656367302   Train Accuracy : 89.608\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  60   Loss:  208.64800308644772   Train Accuracy : 89.966\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  61   Loss:  208.08546522259712   Train Accuracy : 90.076\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  62   Loss:  200.33004915714264   Train Accuracy : 90.512\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  63   Loss:  197.82330645620823   Train Accuracy : 90.586\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  64   Loss:  198.13665986061096   Train Accuracy : 90.504\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  65   Loss:  196.05133871734142   Train Accuracy : 90.714\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  66   Loss:  193.03961797058582   Train Accuracy : 90.82\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  67   Loss:  191.4700836390257   Train Accuracy : 91.114\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  68   Loss:  191.39041478931904   Train Accuracy : 90.95\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  69   Loss:  188.45720049738884   Train Accuracy : 91.2\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  70   Loss:  187.37073402106762   Train Accuracy : 91.236\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  71   Loss:  185.44602762162685   Train Accuracy : 91.558\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  72   Loss:  185.27840465307236   Train Accuracy : 91.43\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  73   Loss:  185.3176045268774   Train Accuracy : 91.25\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  74   Loss:  180.03229123353958   Train Accuracy : 91.762\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  75   Loss:  180.2078735679388   Train Accuracy : 91.78\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  76   Loss:  178.249825745821   Train Accuracy : 91.786\n",
      "\n",
      "\n",
      "Test accuracy: 87 %\n",
      "Epoch:  77   Loss:  175.08260321617126   Train Accuracy : 92.002\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  78   Loss:  175.38579320907593   Train Accuracy : 91.858\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  79   Loss:  163.284510537982   Train Accuracy : 92.962\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  80   Loss:  156.30857780575752   Train Accuracy : 93.392\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  81   Loss:  154.94129614531994   Train Accuracy : 93.448\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  82   Loss:  152.19577187299728   Train Accuracy : 93.576\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  83   Loss:  148.8811552375555   Train Accuracy : 93.722\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  84   Loss:  147.66768811643124   Train Accuracy : 93.774\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  85   Loss:  146.8414444923401   Train Accuracy : 93.87\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  86   Loss:  147.40137347579002   Train Accuracy : 93.764\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  87   Loss:  146.4121664315462   Train Accuracy : 93.836\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  88   Loss:  143.45532520115376   Train Accuracy : 94.114\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  89   Loss:  141.68871469795704   Train Accuracy : 94.158\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  90   Loss:  140.96982376277447   Train Accuracy : 94.246\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  91   Loss:  142.20124973356724   Train Accuracy : 94.068\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  92   Loss:  139.4942869991064   Train Accuracy : 94.232\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  93   Loss:  138.18461886048317   Train Accuracy : 94.21\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  94   Loss:  136.29987278580666   Train Accuracy : 94.49\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  95   Loss:  137.9374244362116   Train Accuracy : 94.274\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  96   Loss:  137.5885872244835   Train Accuracy : 94.394\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  97   Loss:  135.80417221784592   Train Accuracy : 94.522\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  98   Loss:  134.90695476531982   Train Accuracy : 94.586\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  99   Loss:  124.93021000176668   Train Accuracy : 95.194\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  100   Loss:  121.18113581091166   Train Accuracy : 95.446\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  101   Loss:  117.16972837597132   Train Accuracy : 95.728\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  102   Loss:  116.31497795879841   Train Accuracy : 95.842\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  103   Loss:  115.28869654238224   Train Accuracy : 95.786\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  104   Loss:  111.88276218622923   Train Accuracy : 96.032\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  105   Loss:  112.87971878796816   Train Accuracy : 95.934\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  106   Loss:  110.3616179972887   Train Accuracy : 96.046\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  107   Loss:  111.74074629694223   Train Accuracy : 95.916\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  108   Loss:  110.64879504591227   Train Accuracy : 96.044\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  109   Loss:  109.53341200947762   Train Accuracy : 96.17\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  110   Loss:  107.55960096418858   Train Accuracy : 96.316\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  111   Loss:  108.41669799387455   Train Accuracy : 96.176\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  112   Loss:  106.43705574423075   Train Accuracy : 96.278\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  113   Loss:  106.80647230893373   Train Accuracy : 96.394\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  114   Loss:  107.57818311452866   Train Accuracy : 96.122\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  115   Loss:  105.33308432251215   Train Accuracy : 96.328\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  116   Loss:  104.95801311731339   Train Accuracy : 96.412\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  117   Loss:  103.46666404604912   Train Accuracy : 96.488\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  118   Loss:  105.3924173116684   Train Accuracy : 96.326\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  119   Loss:  98.19997533410788   Train Accuracy : 96.89\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  120   Loss:  94.77212131023407   Train Accuracy : 96.96\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  121   Loss:  91.02633254230022   Train Accuracy : 97.24\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  122   Loss:  89.9583016782999   Train Accuracy : 97.432\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  123   Loss:  88.8782869502902   Train Accuracy : 97.404\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  124   Loss:  90.48768936097622   Train Accuracy : 97.284\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  125   Loss:  86.90154327452183   Train Accuracy : 97.578\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  126   Loss:  89.84437128901482   Train Accuracy : 97.34\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  127   Loss:  87.68676115572453   Train Accuracy : 97.458\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  128   Loss:  87.69094812870026   Train Accuracy : 97.474\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  129   Loss:  87.22685981541872   Train Accuracy : 97.454\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  130   Loss:  86.71046245843172   Train Accuracy : 97.46\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  131   Loss:  85.58862669020891   Train Accuracy : 97.556\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  132   Loss:  85.24519249796867   Train Accuracy : 97.55\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  133   Loss:  86.19837851077318   Train Accuracy : 97.468\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  134   Loss:  86.76784279942513   Train Accuracy : 97.472\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  135   Loss:  83.53486328572035   Train Accuracy : 97.584\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  136   Loss:  82.15902803838253   Train Accuracy : 97.678\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  137   Loss:  84.84032775461674   Train Accuracy : 97.682\n",
      "\n",
      "\n",
      "Test accuracy: 88 %\n",
      "Epoch:  138   Loss:  81.37139519304037   Train Accuracy : 97.842\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  139   Loss:  80.08923007547855   Train Accuracy : 97.922\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  140   Loss:  77.14678084105253   Train Accuracy : 98.054\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  141   Loss:  75.33515567332506   Train Accuracy : 98.236\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  142   Loss:  74.69447939097881   Train Accuracy : 98.236\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  143   Loss:  72.94470223784447   Train Accuracy : 98.364\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  144   Loss:  74.99046797305346   Train Accuracy : 98.136\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  145   Loss:  74.90656341612339   Train Accuracy : 98.156\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  146   Loss:  72.32553888857365   Train Accuracy : 98.33\n",
      "\n",
      "\n",
      "Test accuracy: 90 %\n",
      "Epoch:  147   Loss:  71.92834213376045   Train Accuracy : 98.314\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  148   Loss:  71.31197595596313   Train Accuracy : 98.412\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  149   Loss:  72.17636661976576   Train Accuracy : 98.354\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  150   Loss:  71.26312101632357   Train Accuracy : 98.366\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  151   Loss:  70.53344591706991   Train Accuracy : 98.412\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  152   Loss:  70.04698736965656   Train Accuracy : 98.418\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  153   Loss:  69.86098981648684   Train Accuracy : 98.48\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  154   Loss:  69.13215976953506   Train Accuracy : 98.436\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  155   Loss:  69.63265351951122   Train Accuracy : 98.364\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  156   Loss:  71.2796732597053   Train Accuracy : 98.398\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  157   Loss:  69.8697013258934   Train Accuracy : 98.45\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  158   Loss:  69.52401360869408   Train Accuracy : 98.472\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  159   Loss:  68.27371594309807   Train Accuracy : 98.6\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  160   Loss:  66.72500628978014   Train Accuracy : 98.682\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  161   Loss:  65.54188691079617   Train Accuracy : 98.696\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  162   Loss:  65.03018414974213   Train Accuracy : 98.73\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  163   Loss:  65.58079321682453   Train Accuracy : 98.732\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  164   Loss:  64.19593349099159   Train Accuracy : 98.77\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  165   Loss:  64.15971096605062   Train Accuracy : 98.764\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  166   Loss:  64.1019694879651   Train Accuracy : 98.714\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  167   Loss:  63.96226701140404   Train Accuracy : 98.814\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  168   Loss:  64.07102362066507   Train Accuracy : 98.746\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  169   Loss:  63.533478647470474   Train Accuracy : 98.794\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  170   Loss:  63.80893073230982   Train Accuracy : 98.736\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  171   Loss:  62.747249469161034   Train Accuracy : 98.818\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  172   Loss:  62.70637249574065   Train Accuracy : 98.868\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  173   Loss:  63.772483594715595   Train Accuracy : 98.818\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  174   Loss:  62.187436528503895   Train Accuracy : 98.886\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  175   Loss:  62.61741929873824   Train Accuracy : 98.802\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  176   Loss:  61.867417350411415   Train Accuracy : 98.878\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  177   Loss:  60.78876842558384   Train Accuracy : 98.92\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  178   Loss:  61.828501395881176   Train Accuracy : 98.916\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  179   Loss:  61.31974221020937   Train Accuracy : 98.95\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  180   Loss:  60.487935811281204   Train Accuracy : 98.98\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  181   Loss:  59.50560459494591   Train Accuracy : 99.076\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  182   Loss:  58.39566191658378   Train Accuracy : 99.068\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  183   Loss:  58.22835220769048   Train Accuracy : 99.086\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  184   Loss:  58.02062277495861   Train Accuracy : 99.102\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  185   Loss:  59.1075997762382   Train Accuracy : 98.984\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  186   Loss:  59.2162915840745   Train Accuracy : 99.008\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  187   Loss:  58.234947841614485   Train Accuracy : 99.136\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  188   Loss:  58.072625778615475   Train Accuracy : 99.062\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  189   Loss:  57.55133219063282   Train Accuracy : 99.174\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  190   Loss:  58.46980990841985   Train Accuracy : 99.09\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  191   Loss:  57.69248654693365   Train Accuracy : 99.124\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  192   Loss:  58.7254884429276   Train Accuracy : 99.008\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  193   Loss:  58.73881136626005   Train Accuracy : 99.07\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  194   Loss:  56.977643713355064   Train Accuracy : 99.122\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  195   Loss:  57.71194835007191   Train Accuracy : 99.122\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  196   Loss:  56.46291799098253   Train Accuracy : 99.2\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  197   Loss:  56.69939185678959   Train Accuracy : 99.196\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  198   Loss:  57.42557544261217   Train Accuracy : 99.19\n",
      "\n",
      "\n",
      "Test accuracy: 89 %\n",
      "Epoch:  199   Loss:  56.32685634493828   Train Accuracy : 99.222\n",
      "\n",
      "\n",
      "Test accuracy: 90 %\n"
     ]
    }
   ],
   "source": [
    "# ======================== Function to get the test accuracy ===============================================================================\n",
    "def test():\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  model.train(False)\n",
    "  with torch.no_grad():\n",
    "    for i,(images,labels)in enumerate(testloader):\n",
    "      if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "      outputs = model(Variable(images))\n",
    "      labels = Variable(labels)\n",
    "      _,predicted = outputs.max(1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted.eq(labels)).sum().item()\n",
    "    print('Test accuracy: %d %%' % (\n",
    "  100 * correct / total))\n",
    "  return 100*(correct/total)\n",
    "\n",
    "#======================================================= Training =========================================================================\n",
    "num_epochs = 200  # Train for 200 epochs\n",
    "start_epoch = 0\n",
    "\n",
    "total_step = len(trainloader)\n",
    "train_loss = []  # Store the train_loss per epoch\n",
    "test_accuracy = [] # Store the test_accuracy per epoch\n",
    "for epoch in range(start_epoch,num_epochs):\n",
    "  model.train(True)\n",
    "  schedule.step()\n",
    "  epoch_loss  = 0\n",
    "  i_count = 0\n",
    "  acc_total = 0\n",
    "  for i,(images,labels) in enumerate(trainloader):\n",
    "    if torch.cuda.is_available():\n",
    "      images = images.cuda()\n",
    "      labels = labels.cuda()\n",
    "    labels = Variable(labels)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(Variable(images))\n",
    "    loss = criterion(outputs,labels)\n",
    "    epoch_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _,predicted = outputs.max(1)\n",
    "    denom = labels.size(0)\n",
    "    correct = predicted.eq(labels).sum().item()\n",
    "    acc = 100*(correct/denom)\n",
    "    acc_total += acc\n",
    "    i_count = i_count + 1\n",
    "    \n",
    "    #if(i%20 == 0):  # Print the loss per 20 iterations\n",
    "      #print(\"Epoch: \",epoch,\" \",\"Iteration: \",i,\" loss: \",loss.item(),\" Train_iter Accuracy: \",acc)\n",
    "  train_loss.append(epoch_loss)\n",
    "  print(\"Epoch: \",epoch,\" \",\"Loss: \",epoch_loss,\" \",\"Train Accuracy :\",acc_total/i_count) # Print train accuracy per epoch\n",
    "  print('\\n')\n",
    "  test_acc = test()      # Print the test accuracy per epoch\n",
    "  test_accuracy.append(test_acc)\n",
    "  \n",
    "  if(epoch%50 == 0):       # Save the model every 50 epoch\n",
    "    state = {\n",
    "        'model': model.state_dict(),\n",
    "        'acc' : test_acc,\n",
    "        'optim':optimizer.state_dict(),\n",
    "        'epoch' : epoch\n",
    "    }\n",
    "    path = './base_' + 'model_' + str(int(epoch)) +'_' + str(int(test_acc))+'.pth'\n",
    "    torch.save(state,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 90 %\n",
      "90.07\n",
      "Accuracy of plane : 97 %\n",
      "Accuracy of   car : 100 %\n",
      "Accuracy of  bird : 86 %\n",
      "Accuracy of   cat : 65 %\n",
      "Accuracy of  deer : 89 %\n",
      "Accuracy of   dog : 84 %\n",
      "Accuracy of  frog : 90 %\n",
      "Accuracy of horse : 96 %\n",
      "Accuracy of  ship : 95 %\n",
      "Accuracy of truck : 93 %\n"
     ]
    }
   ],
   "source": [
    "#======================================= Testing ===================================================================================================\n",
    "test_acc = test() # Test error\n",
    "print(test_acc)\n",
    "\n",
    "# Per class accuracy\n",
    "class_correct = list(0. for i in range(10)) # Individual class error\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        if torch.cuda.is_available():\n",
    "          images = images.cuda()\n",
    "          labels = labels.cuda()\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
